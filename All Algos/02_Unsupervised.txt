# -----------------------------
# ğŸ“˜ Scikit-learn Unsupervised Algorithms â€“ With Use Cases
# -----------------------------

# 1ï¸âƒ£ Clustering Algorithms
# -------------------------------------------------------
# KMeans                      â†’ When you know the number of clusters (e.g., customer segmentation)
# MiniBatchKMeans             â†’ Faster version of KMeans for large datasets
# DBSCAN                      â†’ For density-based clusters (e.g., anomaly detection in spatial data)
# OPTICS                      â†’ Better for variable density clusters
# MeanShift                   â†’ Automatically finds number of clusters; slow
# AgglomerativeClustering     â†’ Hierarchical clustering (e.g., gene expression analysis)
# Birch                       â†’ Very large datasets with many clusters
# SpectralClustering          â†’ Non-convex clusters or graph-based data

# 2ï¸âƒ£ Dimensionality Reduction
# -------------------------------------------------------
# PCA (Principal Component Analysis) â†’ Reduce dimensions while preserving variance (e.g., image compression)
# TruncatedSVD                      â†’ PCA for sparse data (e.g., text, LSA in NLP)
# NMF (Non-negative Matrix Factorization) â†’ Topic modeling for non-negative data (e.g., documents)
# t-SNE                             â†’ Visualization of high-dimensional data (e.g., clustering results)
# UMAP                              â†’ Faster alternative to t-SNE with better global structure

# 3ï¸âƒ£ Manifold Learning (Nonlinear Dimensionality Reduction)
# -------------------------------------------------------
# Isomap                          â†’ Captures nonlinear structures (e.g., motion trajectory modeling)
# LocallyLinearEmbedding (LLE)   â†’ Local geometry preservation
# MDS (Multi-Dimensional Scaling)â†’ Preserves pairwise distances (e.g., perceptual similarity)

# 4ï¸âƒ£ Gaussian Mixture Models
# -------------------------------------------------------
# GaussianMixture                â†’ Probabilistic soft clustering (e.g., customer behavior profiles)
# BayesianGaussianMixture       â†’ Automatically selects number of components

# 5ï¸âƒ£ Anomaly Detection
# -------------------------------------------------------
# IsolationForest                â†’ Efficient for high-dimensional anomaly detection (e.g., fraud, outliers)
# OneClassSVM                    â†’ For novelty detection (e.g., identifying unseen network attacks)
# EllipticEnvelope               â†’ Detect outliers assuming Gaussian distribution

# -----------------------------
# âœ… Summary Table â€“ When to Use What
# -----------------------------
# Clustering              â†’ Discover hidden groups without labels
# Dimensionality Reduction â†’ Reduce input dimensions before modeling or for visualization
# Manifold Learning       â†’ Preserve nonlinear structure of data in fewer dimensions
# Mixture Models          â†’ Probabilistic clustering, soft memberships
# Anomaly Detection       â†’ Identify rare/unusual patterns in data

# -----------------------------
# ğŸ§  Typical Use Cases by Industry
# -----------------------------
# Retail/Ecomm   â†’ Customer segmentation (KMeans), behavior clustering (DBSCAN)
# Finance        â†’ Anomaly detection (IsolationForest), risk groupings (GaussianMixture)
# Healthcare     â†’ Gene clustering (Agglomerative), disease subtypes
# NLP/Text       â†’ Topic modeling (NMF), semantic grouping (t-SNE)
# IoT/Network    â†’ Outlier detection (OneClassSVM), event detection

# -----------------------------
# ğŸ“Œ Notes:
# - Use PCA or TruncatedSVD before clustering for high-dimensional data.
# - DBSCAN and OPTICS do not require the number of clusters in advance.
# - t-SNE/UMAP are best for 2D/3D visualizations only, not for modeling input.
